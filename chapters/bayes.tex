\chapter{Bayes parameter extraction of complex model}
In the past three chapter, we have discussed the modeling details of the heavy flavor transport in the environment of the relativistic heavy-ion collisions.
And it is evident that the simulation requires a complex model with complex inputs.
To use such a model to acquire quantitative understanding about the properties of the quark-gluon plasma, we apply advanced statistical tools known as the Bayes analysis.
Let me first list out the problems one encounters in the model-to-data comparison in the field of heavy-ion collisions.

\paragraph{Complex dynamical model with high-dimensional inputs}
Taking the bulk model as an example, it includes an initial condition, a dynamical model for pre-equilibrium stage, the viscous hydrodynamics, particlization of the hydrodynamic energy momentum tensor and the hadronic decays and rescatterings.
Each of the module takes both physical parameters and modeling parameters.
Eventually, one can have more than 10 parameters.
To generate one event with a model itself is already computationally intensive, while to be able to compare to experimental measurements that average over centrality with a good control of statistical uncertainty, $O(10^4)$ minimum-biased events needed to be generated.
As a result, a na\"ive linear grid sweep over the 10-dimensional parameter space is certainty not feasible.
To solve this problem of high-dimensional input complex models requires advanced parameter set design method and reliable interpolation schemes.

\paragraph{Model uncertainty and correlation among parameters}
We are mostly interested in those ``physical parameters / quantities'', for example, the transport coefficients $\eta/s(T)$ and $\zeta/s(T)$ of the medium.
Other parameters, such the switching time between the pre-equilibrium free-streaming dynamics and the hydrodynamics, and the temperature at which one particlize a hydrodynamic field description into a hadronic ensemble are less interested in the physical sense.
The appearance of these matching parameters are simply due to we do not have a single ``global model'' that works reasonably well under all circumstances.
However, one should not think less of these model parameters, because the quantify a significant fraction of the modeling uncertainty.
The model uncertainty can affect the interpretation of the experimental data, and any quantitative statement we draw about those physical quantities from a model-to-data comparison.
For example, the  bulk particles anisotropic flows, though proved to be very sensitive to the shear viscosity, are also sensitive to the initial eccentricity of the initial conditions.
Therefore, the exacted $\eta/s$ becomes correlated with the choice of initial condition models and its parameters.
The model uncertainty is hard problem, and one can either try to reduce it by improving the model or quantify it when compared to data to prevent an over-interpretation.

\paragraph{A high-dimensional model output / observables}
The inclusion of more observables also helps to break the model degeneracy and parameter correlation.
But it can be tricky to quantify the quality of agreement between model and data with a collection of observables (a high dimensional output).


The Bayesian analysis framework is introduced to the field by the earily works of and the PhD thesis of Jonah Bernhard [] who has successfully applied the tool to the extraction of the initial condition topology and temperature dependent QGP transport coefficient.
This chapter provides a concise description of the Bayesian analysis and for full details please refer to the thesis of Jonah Bernhard [].

We abstract the general task of a model-to-data comparison into the following form,
\begin{itemize}
\item A complex model $M$, with input parameters organized as a vector $p$ of dimensional $m$.
\item There are certain prior belief on the reasonable range of each parameters, known as the prior probability distribution $\mathrm{Prior}$.
\item The experimental data are organized as an observation vector $y_{\exp}$ of dimensional $n$
\item Goal: to infer the probability distribution of $p$, given the model $M$, the measurements $y_{\exp}$, and the prior belief $\mathrm{Prior}$. The resultant distribution is called the posterior probability distribution $\mathrm{Posterior}$.
\end{itemize}

\section{The design of parameters}
The first step is an understanding of the behavior of the model $M$.
This is done by sampling the high-dimensional input parameter space with a finite number of design parameter set.
Mathematically, this $N$ set of parameter vectors of length $n$ forms a so-called design matrix $\mathbf{D}$,
\begin{eqnarray}
\mathbf{D}_{N\times n} = 
\begin{bmatrix}
p_{11} & p_{12} & \cdots & p_{1n}\\
p_{21} & p_{22} & \cdots & p_{2n}\\
\vdots & \vdots & \ddots & \vdots \\
p_{N1} & p_{N2} & \cdots & p_{Nn}
\end{bmatrix}
\end{eqnarray}
where the first index is the label of different parameter set, and the second index labels different parameters.
We want an efficient sampling of the design points, so that the number of $N$ is manageable while still provide a representative coverage of the entire parameter space.
We used Latin-Hyper-Cube sampling method, it generate a random design but subject to the following constraint:
\begin{itemize}
\item The marginalized distribution on any parameters is a uniform distribution. This is different from a grid design, where the marginalized distribution are spiky delta function on the grid points.
\item The minimum distanced between any two point in the parameter space is maximized. This is different from a complete random design, where the points may from tight cluster or sparse regions.
\end{itemize}
Usually, for a well behaved model, the number of design points needed for a good interpolation increases linearly with the number of parameters, in contrast to the exponential increase with number of parameters for a grid design.

Once the design is made, the time consuming part is then to perform the full model calculation on each points and organize all the computed observables into an observation matrix,
\begin{eqnarray}
\mathbf{Y}_{N\times m} = 
\begin{bmatrix}
y_{11} & y_{12} & \cdots & y_{1m}\\
y_{21} & y_{22} & \cdots & y_{2m}\\
\vdots & \vdots & \ddots & \vdots \\
y_{N1} & y_{N2} & \cdots & y_{Nm}
\end{bmatrix}
\end{eqnarray}
where the first index is the label of different parameter set, and the second index labels different observables.
The design matrix $\mathbf{D}$ and the observations matrix $Y$ forms the training data to build a general interpolator for the mapping from the parameter space to the observable space $M: \vec{p} \rightarrow \vec{y}(\vec{p})$.

\section{Data reduction}
The model $M$ is an $n$-dimesional vector to an $m$-dimensional vector mapping. 
One can certainly construct an independent array of $m$ scalar mappings, and interpolate each of them using the training data.
However, this na\"ive construction does not make use of the intrinsic correlations / structures that is already presented in the training data, and can be very inefficient for practice usage.
For example, consider the observations contain two values of $R_{AA}$ and $v_2$ for $D$-meson, and usually the larger the $R_{AA}$ is, the smaller the $v_2$, and thus an anti-correlation is expected.
If one build interpolators for $R_{AA}$ and $v_2$ independently, their uncertainty is also going to be independent, and the intrinsic correlation is overlooked.
However, if one choose to interpolates the linearly combinations $a R_{AA} \pm b v_2$, then a wise choice of $a, b$ can largely reduces the correlation between these two ``new" observables.

The principal component analysis (PCA) is the systematic way to implement this idea.
The original vectors of observables are transformed into the principal-component (PC) space, with each PC a specific linear combination of the original observables, so that the covariances between the newly defined observables (the PCs) vanish.
Mathematically, this is the same as finding the singular value decomposition (SVD) of $\mathbf{\tilde{Y}}$. 
$\mathbf{\tilde{Y}}$ is the standardized observation matrix $\mathbf{Y}$,
\begin{eqnarray}
\tilde{y}_{ij} = \frac{y_{ij} - \mu_j}{\sigma_j}
\end{eqnarray}
with $\mu_j$ and $\sigma_j$ the mean and the standard deviation of column $j$.
Then the SVD proceeds as,
\begin{eqnarray}
\tilde{\mathbf{Y}} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}
\end{eqnarray}
And the principal components are defined as the components after the $V$ transformation.
\begin{eqnarray}
z = \mathbf{V}y
\end{eqnarray}
It is evident that the covariance matrix of the $z$ observables are diagonalized,
\begin{eqnarray}
\mathrm{Var}(z_i, z_j) = \frac{1}{N}V_{ii'}\tilde{Y}_{ki'}V_{jj'}\tilde{Y}_{kj'} = \frac{1}{N}V\tilde{Y}^T\tilde{Y}V^T = \frac{1}{N}\mathbf{\Sigma}
\end{eqnarray}
so that the PCs are orthogonalized.
One more benefits is that, suppose the variance in $\Sigma$ has been ordered from maximum to minimum.
For data with pronounced structures, often the first few principal components take account the majority of the data variance.
And for practical usage, a truncation of the number of PCs already gives a good representation of the original data, and this greatly reduces the computations for large number of observables.
In the end, one can always go back from the PC space to the original physical space by $y = V^{-1} z$.

\section{Model emulator}
With limited information on a finite number of design points contained in the matrices $D$ and $M$, the original mapping is approximated by a model emulator (a surrogate model) using the Gaussian Process (GP).
The Gaussian Process provide a non-parametric interpolation for scalar function and also works for high dimensional input.
We shall let the readers refer to [] for the technique details related to the GP and only summarize the basic ideas.

Taking a uni-variate case as an example, given an array of input and an array of scalar output, the common way to interpolate the the data are, e.g., polynomial interpolation.
However, polynomial interpolation only uses local information of the grid, and its performance can be sensitive to the error of the output (e.g. statistical fluctuation in the measurement / simulation).
Moreover, it is hard to generate to a high-dimensional Lain-hypercube design because the design points are not arranged on a grid.
A GP does not make any assumption on the function form of the interpolation, but infer the output at a given input point based on how the output at the present point correlates with other points that has been given.
Mathematically, one assumes that the array of output $\vec{y}^*$ to be predicted at input $\vec{x^*}$ forms a multi-variate normal distribution with the output $\vec{y}_{\textrm{train}}$ at the training points $\vec{x}_{\textrm{train}}$,
\begin{eqnarray}
\begin{bmatrix}
\vec{y}^* \\
\vec{y}_{\textrm{train}}
\end{bmatrix}
\sim
\mathcal{N}\left(
\begin{bmatrix}
\mu^* \\
\mu_{\textrm{train}}
\end{bmatrix},
\begin{bmatrix}
\mathbf{\Sigma}(\vec{x}^*, \vec{x}^*)& \mathbf{\Sigma}(x^*, x_{\textrm{train}}) \\
\mathbf{\Sigma}(\vec{x}_{\textrm{train}}, \vec{x}^*)& \mathbf{\Sigma}(\vec{x}_{\textrm{train}}, \vec{x}_{\textrm{train}})
\end{bmatrix}
\right)
\end{eqnarray}
Here, without a loss of generality the mean vectors $\mu^*$ and $\mu_{\textrm{textrm}}$ are often set to zero.
The $\mathrm{\Sigma}$'s forms the co-variance matrix, and each of them has the same shape of the outer product of its two argument vectors.
Its matrix-element (the kernel function) often takes a squared exponential form,
\begin{eqnarray}
\Sigma_{ij} = k(x_i, x_j) = \sigma^2 \exp\left(-\frac{(x_i-x_j)^2}{2l^2}\right) 
\end{eqnarray}
Where the auto correlation is $\sigma^2$ at the same input, and correlation decays exponentially with squared separation of the input points.
In such a way, points that are close in inputs will also be close in outputs.

Because, we have known the the value of the training output for sure, one obtains the probability distribution of $\vec{y}^*$ by conditioning the training output on a set of fixed values,
\begin{eqnarray}
\vec{y}^* \sim &&\mathcal{N}\left(
\mathbf{\Sigma}(\vec{x}^*, \vec{x}_{\textrm{train}} )
\mathbf{\Sigma}^{-1}(\vec{x}_{\textrm{train}}, \vec{x}_{\textrm{train}} )\vec{y}_{\textrm{train}},\right.\\\nonumber
&&\left.
\mathbf{\Sigma}(\vec{x}^*, \vec{x}^*) - 
\mathbf{\Sigma}(\vec{x}^*, \vec{x}_{\textrm{train}} )
\mathbf{\Sigma}^{-1}(\vec{x}_{\textrm{train}}, \vec{x}_{\textrm{train}} )
\mathbf{\Sigma}(\vec{x}_{\textrm{train}},\vec{x}^*)
\right)
\end{eqnarray}
Note that the conditional multivariate normal distribution is still a normal distribution, with modified mean and covariance matrix.
One can easily check that if the predicting input approaches one of the training input, the distribution of the output approaches an delta function (as the limit of a narrow Gaussian) at the training output.

\paragraph{Inference with uncertainty quantification} The GP does not provide a single estimation of the out, as does the polynomial interpolation, but infer the probability distribution of the predicted outputs by giving both the mean estimation and the covariance.
This is a huge advantage of the Gaussian Process as it quantifies its own interpolation uncertainty.
The variances of the predicted outputs are the diagonal elements of the covariance matrix.

\paragraph{Hyperparameters and training} We have not discussed how to determine the value of the parameters in the kernel function $k$ yet, which are the variance $\sigma^2$ and the correlation length. 
The squared exponential form is not the only possible kernel function, more sophisticated choices with more parameters are designed for varies problems.
These extra parameters in the kernel function are known as hyper-parameters (denoted as a vector $\vec{\theta}$), and should in principle, also be treated as unknown parameters in the calibration.
However, a common practice to reduce the complexity by fixing these of hyper-parameters at a set of ``optimal values'' by minimize the loss function $\mathcal{L}$ of the training processes,
\begin{eqnarray}
\mathcal{L} = -\ln p(\vec{y}|\vec{\theta}) = \frac{1}{2}\ln \det \mathbf{\Sigma}(\vec{\theta})  + \frac{1}{2}\vec{y}^T \mathbf{\Sigma}(\vec{\theta})^{-1} \vec{y} + \frac{N}{2}\ln(2\pi)
\end{eqnarray}
where $\vec{y}$ is the (PCA transformed) training data, and $N$ is the number of training points.
By minimizing this quantity, an optimal level of interpolation is achieved.
This minimization process of fixing the hyper-parameters in the GP emulators is called a ``training'' processes.

\paragraph{Validation} Though the training process includes certain penalty for over-fitting the data, whether the trained GP has over-fitting problem can only be checked validation.
An validation can be done by performing the model calculation at novel points in the parameter space that is not ``learned" by the GP, and compare the trained GP's prediction $y_i \pm \sigma_i$ to the model calculation $y_{\textrm{validate}, i}$.
If an emulator is trained to work properly, then the standardized deviation $(y_i - y_{\textrm{validate}, i})/\sigma_i$ should be approximately a standard normal distribution.

\paragraph{Multivariate inputs and outputs} The GP formulation can be generalized to higher dimensional inputs easily by specifying a multidimensional kernel function.
For high dimensional outputs, one first applies the PCA analysis introduced in the previous section and the build individual GPs for each of the first $N_{PC}$ principal components that take most of the data's variance.

\section{Bayes' theorem and Markov chain Monte Carlo}
With the model emulator $M$ (we are using the same symbol as the model, but one should always remember that the emulator is only a fast surrogate of the original model and comes with uncertainty), we proceed to the essential of the statistical analysis, which is the Bayes' theorem.
The Bayes' theorem is the quantitative way to update the knowledge of model parameters with empirical observations,
\begin{equation}
\mathrm{Posterior}(\vec{p}|M, \vec{y}_{\textrm{exp}}) \propto \mathrm{Likelihood}(\vec{y}_{\textrm{exp}}|M, \vec{p})\times\mathrm{Prior}(\vec{p}).
\end{equation}
It states that the posterior probability distribution of parameters, given model and experimental measurements, is proportional to the likelihood to describe the experiments with the model using this set of parameters, times the prior belief of the distribution of the parameters.
The likelihood is function is often assumed to be a multivariate Gaussian,
\begin{eqnarray}
\mathrm{Likelihood}(\vec{p}) &=& (2\pi)^{-\frac{m}{2}} (\det|\Sigma|)^{-\frac{1}{2}} \exp\left\{-\frac{1}{2}\Delta \vec{y}^T \mathbf{\Sigma}^{-1} \Delta \vec{y}\right\}, \\ 
\Delta \vec{y} &=& \vec{y}(\vec{p}) - \vec{y}_{\textrm{exp}}
\end{eqnarray}
where the $\vec{y}(\vec{p})$ is the model emulators' prediction at parameter point $\vec{p}$, $m$ is the number of observables.
The prior distribution is often a multi-dimensional uniform distribution within a reasonable range. 
The covariance matrix contains various sources of uncertainty from both theory and experimental side.

\paragraph{A model dependent statement} first one notice that the posterior is always defined with a given model, and therefore even the extraction of theoretically well defined quantities can be affected by the use of different dynamical modeling.
The ultimate solution is of course the improvement of model's physical accuracy.
Or using a flexible model or models with different (but reasonable) assumptions to extract the same quantity and estimate the level of theoretical uncertainty.

\paragraph{The covariance matrix} covariance matrix is decomposed into different contributions which will be briefly introduced
\begin{eqnarray}
\mathbf{\Sigma} = \mathbf{\Sigma}_{\textrm{stat}} + \mathbf{\Sigma}_{\textrm{sys}} + \mathbf{\Sigma}_{\textrm{emu}} + \mathbf{\Sigma}_{\textrm{trun}}
\end{eqnarray}
\begin{itemize}
\item The statistical covariance  takes the diagonal form, $\mathbf{\Sigma}_{\textrm{stat}} = \delta_{ij}\sigma_{\textrm{stat}, i}^2$. And $\sigma_{\textrm{stat}, i}$ is the experimental statistical uncertainty.
\item The experimental systematic uncertainty can correlation across different $p_T$ bins and different centrality. However, due to the lack of information on how these correlation looks like, we parametrize the correlation to a Gaussian type in the $\ln(p_T)$ space, and a constant factor for different centrality,
\begin{eqnarray}
\mathbf{\Sigma}_{\textrm{stat}} = \sigma_{\textrm{sys}, i}\sigma_{\textrm{sys}, j} C_{ij} \exp\left\{-\frac{1}{2 L_{\textrm{corr}}^2} \left(\ln\frac{p_{T, i}}{p_{T, j}}\right)^2 \right\}.
\end{eqnarray}
$L_{\textrm{corr}}$ is a the correlation length. 
$C_{ij}$ is unity if the quantities $i$ and $j$ are from the same centrality of the same observables; it is zero if they are different observable. 
For $R_{AA}$, $C_{ij}$ is a constant between $0$ and $1$ for different centrality. This mimics the fact that $R_{AA}$ from different centrality always uses the same $p$-$p$ baseline measurements and therefore has a fraction of its uncertainty correlated.
For momentum anisotropy, $C_{ij}$ is still zero for between different centralities.
We can only guess the $\ln p_T$-correlation length and centrality correlation right now which introduces an ambiguity in the model-to-data comparison.
We hope that future measurements will provide more information on the covariance structure of the published systematic error bars.
\item The emulator covaraince $\mathbf{\Sigma}_{\textrm{emu}}$ is the prediction covariance of the GPs in the PC space and then transformed into the physical space.
\item Finally, the truncation covaraince $\mathbf{\Sigma}_{\textrm{trun}}$ take those less important principal components that are not being emulated by GPs into account. These fraction of the data variance is computed in t the PC space with a diagonal form and transformed back to the physical space.
\end{itemize}

\paragraph{Marginalize the posterior distribution} The resultant posterior distribution a function of $n$-parameter spaces.
To answer questions such as what is the probability distribution of one parameter folded with uncertainty from other parameter, one looks at the marginalized distribution with the other $n-1$ parameters integrated out.
This is done by a Markov chain Monte Carlo (MCMC) sampling of the posterior function and obtains an ensemble of $n$-dimensional walkers whose distribution thermalizes into the posterior distribution.
The maginalized distribution is then the distribution of the projected ensemble onto one dimension.
Similary, a marginalization of the joint distribution of two or more parameters can be obtained similarly.



