\chapter{Introduction of relativistic heavy-ion collision}
\section{Quantum chromodynamics and its phase-diagram}
The Quantum Chromodynamics (QCD) is the fundamental theory of the strong interaction.
It describes the interaction of objects that carries color charges, with
quarks and gluons as elementary degrees of freedom. 
Quarks are fermions and gluons are bosons that mediate the interaction between color charges.
The QCD Lagrangian (with one flavor of quark) is,
\begin{eqnarray}
\mathcal{L} = \bar{\psi_i} \left(i\gamma_\mu D^\mu_{ij} -m \delta_{ij} \right)\psi_j - \frac{1}{4}G_{\mu\nu}^a G^{\mu\nu,a},
\end{eqnarray}
where $\psi_i$ the Dirac spinor of the quark field with $i$ the color index.
\begin{eqnarray}
D_{ij}^\mu = \partial^\mu - i g T_{ij}^a A^{\mu, a}
\end{eqnarray}
is the covariant derivative, containing the interaction between quark field and the gluon field with coupling strength $g$.
Here $T_{ij}^a$ are the generators of the SU(3) group in the fundamental representation and the generators satisfies the commutation relation,
\begin{eqnarray}
[T^a, T^b] = i f^{abc} T^c
\end{eqnarray}
where $f^{abc}$ are known as the structure constants of SU(3).
The field tensor of gluon with color $a$ is,
\begin{eqnarray}
G^{\mu\nu,a} = \partial^\mu A^{\nu, a} - \partial^\nu A^{\mu, a} + g f^{abc} A^{\mu,b}A^{\mu,c}
\end{eqnarray}
The first term is similar to kinetic term of the photon of quantum electrodynamics.
The second term of gluon self-interation (also with strength $g$) is a unique feature of the non-Abelian gauge field.

\begin{figure}
    \centering
    \includegraphics{qcd-feynman-rules.png}
    \caption{The Feynman rule of QCD}
    \label{fig:qcd-feynman}
\end{figure}

Due to quantum fluctuations, the coupling constant of the quantum field theory runs with the resolution, i.e., energy scale. 
The running of the $g$ is characterized by the $\beta$-function, which can be calculated order by order in perturbation theory (please refer to figure \ref{fig:qcd-feynman} for the Feynman rules), 
\begin{eqnarray}
\frac{\partial g}{\partial \ln\mu} = \beta(g).
\end{eqnarray}
At leading order, the QCD $\beta$ function with number of colors $N_c$ and $n_f$ flavors of fermions is,
\begin{eqnarray}
\beta(g) = - \left( \frac{11}{3}N_c - \frac{2}{3}n_f \right) \frac{g^3}{16\pi^2}.
\end{eqnarray}
This $\beta$ function is negative for QCD ($N_c=3$) with realistic number of flavors of quarks $n_f = 2\cdots 6$, meaning the effective coupling constant decreases with increasing energy resolution.
This is known as the asymptotic freedom of QCD that its interaction becomes small at asymptotically high energy.
It is the asymptotic freedom that made the usage of perturbation theory at high energy possible.
One usually defines the strong coupling constant in terms of $\alpha_s = g^2/4\pi$.
Using the leading order $\beta$-function, its scale dependence can be solved as,
\begin{eqnarray}
    \alpha_s(Q^2) = \frac{4\pi}{\left(\frac{11}{3}N_c - \frac{2}{3}n_f\right)\ln\left(\frac{Q^2}{\Lambda^2}\right)}
\end{eqnarray}
With the integration constant absorbed into the QCD energy scale $\Lambda$.
Therefore $\Lambda$ becomes the only parameter at least in perturbative QCD, and can only be determined by experimental measurements. 
For example, in figure \ref{fig:alphas}, the scale dependences of $\alpha_s$ are measured by different experiments.
This data is nicely described by the perturbative running coupling, once its value is fixed at certain energy scale, i.e., at the mass of the $Z$ boson such that $\alpha_s(M_z) = 0.1185$.
For the leading order running, this is $\Lambda \approx 200$ MeV.

\begin{figure}
    \centering
    \includegraphics{alphas.png}
    \caption{Caption}
    \label{fig:alphas}
\end{figure}

At high energy, the decreasing of the effective coupling constant is logarithmic slow, but at low energy the perturbative coupling rises quickly approaching $\Lambda$.
Before reaching this scale, the coupling constant is already too large for reliable perturbative calculations, and $\Lambda$ often serves as an estimation of the QCD non-perturbative scale.
The non-perturbative physics cannot be obtained from perturbative QCD and the only reliable way nowadays is lattice QCD technique where the QCD Lagrangian is discretized on a finite lattice and studied on a computer.
Experimentally, we know that at large distance $L\sim 1/\Lambda$, the coupling between color charges is so strong that isolated color never exists alone without the strong color field constantly populating more colored objects.
These color charge carriers combine to form colorless QCD bound states known as hadrons, such as protons, neutrons and pions.
Depending on its valance quarks (quarks that carry the net quantum number) content, hadrons are generally categorized as baryons and mesons.
Baryons are hadrons with three valance quarks or anti-quarks, while mesons consist of a valance quark and an anti-quark.
Hadrons are also populated with sea-quarks and gluons that are constantly produced in quantum fluctuations.
The hadron momentum is mostly carried by the valance quaks, while the sea quarks and gluons only share a fraction, but the later is very important for particle production in relativistic hadron collisions.

Because of asymptotic freedom property of QCD at high energy, people have been questing for the phase structure of nuclear matter under extreme conditions and related phase transitions.
The phase-diagram is often studied with temperature and baryon chemical potential as independent variables. 
The phase structure on the landscape of independent thermodynamic quantities such as isospin asymmetry, strangeness, etc have also attract great attentions.
Ordinary nuclear matter (nuclei) are many body bound states at zero temperature and Baryon chemical potential $\mu_B\approx 1$ GeV.
A first insight at finite temperature is that because the weakening of the coupling at high energy scale, the nuclear matter was thought to transit from hadronic matter to a system of liberated quarks and gluons, termed the quark-gluon plasma (QGP). 
Lattice QCD calculations have studied this transition at zero baryon chemical potential with physical parameters.
Figure \ref{fig:qcd_eos} quotes the results obtained from the HotQCD Collaboration.
It shows the pressure, energy density and entropy density of the 2+1 flavor QCD.
The thermodynamic quantities scaled by powers of temperature can be understood as the effect number of degrees of freedom.
The non-interacting limit (ideal gas of quarks and gluons) is denoted as the dashed lines. 
It is observed that the lattice results converges to the expectation from a hadron resonance gas model (HRG) at low temperature and smoothly transit to higher values in a narrow temperature range around the pseudo critical temperature $T_c \approx 150 $ MeV (1.5 trillion Kelvin), suggesting an increase of degrees of freedom.

\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{qcd-eos.png}
    \caption{Caption}
    \label{fig:qcd_eos}
\end{figure}

This transition is a cross-over type phase transition at vanishing baryon chemical potential.
At finite baryon chemical potential, the lattice QCD is faced with the fermion sign problem. But many phenomenological models and Dyson-Schwinger equations studies have suggested the existence of a first order phase transition at large $\mu_B/T$.
If such a first order phase transition exist, the coexistence line must end at some point when decreasing $\mu_B/T$, approaching the cross-over phase transition established from lattice study.
Such a point, called the critical end point (CEP), is of great interest both theoretically and experimentally.
At high enough chemical potential (density) and low temperature, another phase of nuclear matter known as the "Color Superconductor" is proposed, where the quarks forms Cooper pairs as those found among electrons in common superconductors.

\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{phase-diagram.png}
    \caption{Caption}
    \label{fig:phase-diagram}
\end{figure}

It is believed that the QCD high temperature phase-transition is a stage of the early evolution of the universe, when the temperature drops down to the transition temperature around a microsecond after the Big Bang.
Compact starts is another celestial testing ground of QCD matter, the high density and low temperature QCD equation of state is an important physical input for simulating the recently discovered gravitational wave from neutron star mergers.
Within lab, the current approach of creating hot and dense nuclear matter is the collision of heavy nuclei at ultra-relativistic high energies, and will be introduced in the next section



\section{Relativistic heavy-ion collision}
Relativistic heavy-ion collision is currently the only tool to access high energy density QCD medium in laboratory.
Since 2005, the Relativistic Heavy-ion Collider (RHIC) at the Brookheaven National Laboratory (BNL) started colliding gold nuclei at 200 GeV. 
The Large Hadron Collider (LHC) started its heavy ion programs later, colliding lead nuclei at 2.76 TeV and 5.02 TeV.
Since then, many evidences have been pointing to the discovery of the new state-of-matter: the quark-gluon plasma.
Three key evidences that lead to this discovery are the observation of strong collective flows of bulk matter, parton recombination as manifest in constituent quark number scaling laws, and the jet quenching phenomenon. 
In this section, after a short introduction of the basic concepts and terminology used in heavy-ion collisions, I will review these three discovery and how these information can be used to understand the property of the quark-gluon plasma.

\subsection{Basics of nuclear collisions}
\paragraph{Kinematics} For the case of ultra relativistic collisions, it is advantages to use a new set of coordinates that is related to the Cartesian coordinates by,
\begin{eqnarray}
x_\perp &=& x_\perp\\
\tau &=& \sqrt{t^2 - z^2}\\
\eta_s &=& \frac{1}{2}\ln\frac{t+z}{t-z}
\end{eqnarray}
where the $z$ direction aligns with the accelerated beam direction.
$\tau$ is called the ``proper time" and $\eta_s$ called the space-time rapidity.
One advantage of using this set of coordinates is that $\tau$ and $\eta_s$ transforms much simpler than $t$ and $z$ under Lorentz boost ($\beta_z$) in the beam direction,
\begin{eqnarray}
\tau' &=& \tau,\\
\eta_s' &=& \eta_s + \frac{1}{2}\ln\frac{1+\beta_z}{1-\beta_z}
\end{eqnarray}
Similarly for momentum, the four components of $p^\mu$ can be transformed into,
\begin{eqnarray}
p_\perp &=& p_\perp\\
m_T &=& \sqrt{E^2 - p_z^2}\\
y &=& \frac{1}{2}\ln\frac{E+p_z}{E-p_z}
\end{eqnarray}
where $m_T$ is called transverse mass, and $y$ the rapidity of a particle.
Besides, pseudorapidity is often used experimentally,
\begin{equation}
\eta = \frac{1}{2}\ln\frac{|p|+p_z}{|p|-p_z} = \frac{1}{2}\ln\frac{1+\cos\theta}{1-\cos\theta}
\end{equation}
It has the merits that it is directly related to the polar angle of final state particles.
When the transverse mass is small compared to $p_z$, the pseudorapidity is also a good proxy of rapidity.

\paragraph{Centrality as a geometry handle of nuclear collisions}
Nuclei are extended objects.
The heavy nuclei radius is approximately scales like $A^{1/3}$ fm, where  $A$ is the atomic number. 
For example, the the radius of a gold and a lead nuclei is about $6-7$ fm upto some diffuseness in the boundary.
In the center-of-mass frame,  nuclei ``shrink" in the $z$ direction by the factor $\gamma = (1-v^2)^{-1/2} = E/M$ because of the Lorentz contraction.
$\gamma$ is a large number, it is over $100$ for gold nuclei at top RHIC energy can be $2500$ for lead nuclei at the LHC.
Therefore, the tow nuclei contracted into thin `pancakes` while approaching each other.

Defining the impact parameter $\vec{b}$ as the transverse separation between the center-of-mass of the two approach nuclei,
the geometry of the collisions can therefore fluctuate from a completely overlapping one ($b=0$) to very peripheral interaction (large $b \gtrsim 2R$).
We will see later that the geometry of a collision event is a useful handle to study the property of its dynamical evolution, however, it is impossible perform high energy experiments with only certain ranges of the impact parameter. 
What is used as an approximate collision geometry indicator in nuclear-nuclear collisions is the so-called centrality.
Centrality can be defined in different ways (multiplicity / transverse energy) and with different kinematic cuts, but the idea is to reflect the nuclear collision geometry by particle production activity.
It is reasonable to anticipate that the average number of charged particles produced or the total transverse energy deposited within a certainty detector acceptance are higher if the collision is more central (small impact parameter), and are lower for peripheral collision.
The relation between centrality and geometry is not exact, because fluctuation in particle production smears out one-to-one correspondence between the impact parameter an the ``centrality meter''.

Experimentally, a minimum-biased sample of recorded events are sorted according to the centrality criteria (multiplicity, e.g.) and the events are binned by percentile.
Then, for example, the top 0--5\% highest multiplicity events are associated to the central collisions with centrality range 0--5\% through a model.
Usually, the models is one of the many variants of the Monte-Carlo Glauber model (to be explained in section \ref{simulation}), which samples the  number of binary nucleon-nucleon collisions and number of participants (nucleons that suffers at least one binary collisions) at a given impact parameter.
It is found that the bulk particle production scales roughly like number of participants, while the cross-section of hard processes (processes involves large momentum transfer $Q \gg \Lambda$).
It is true that the centrality definition can be model dependent, but the uncertainty can be quantified and its prediction can be validated by the production of colorless probes.

\section{Learn about QGP from soft observables}
Experimentally, it is found that the particle produced in relativisitc nuclear collisions has a wide pseudo-rapidity distribution, and a steep falling transverse momentum spectra.
The bulk of the particles in one event are relatively soft with $p_T \lesssim 3$ GeV.
One of the most striking discovery from RHIC and LHC heavy-ion program is that these soft particles depicts a strong collectivity, and hydrodynamic based models are able to describe the bulk observables to a very high precision.
This success of the hydrodynamic model reveals the strongly coupled nature of the matter produced with a temperature several times above $T_c$ and it has been entitled the name strongly coupled quark-gluon plasma (sQGP).
This is in contrary to the previous expectation of a weakly coupled gas of quarks and gluons.

\subsection{Anisotropic flow and QGP transport coefficient} 
One of the manifestation of collectivity is the momentum space anisotropy of the final state. 
Decomposing the charged particle spectra into Fourier series of the azimuthal angle,
\begin{eqnarray}
E\frac{dN}{p_T dp_T d\phi dy} = \frac{1}{2\pi}\frac{dN}{p_T dp_T dy}\left(1 + \sum_{n=1}^{\infty}v_n(p_T)\cos\left[n(\phi-\Psi_n)\right]\right).
\end{eqnarray}
The first term in the expansion is azimuthal angle averaged yield.
The terms in the sum expands the angular dependence. 
The $n=1$ term is a movement of the center-of-mass.
From $n=2$, a non-zero $v_n$ characterize the anisotropy in momentum-space of various order.
If the particle production in high energy nuclear collisions is simply an independent sum of elementary nucleon binary collisions, then the anisotropy would be zero after averaging over many events. 
However, experiments observe a surprisingly larger elliptic flow ($v_2$) in Au+Au collisions and later in Pb+Pb collisions.
Higher order flows harmonics and in particular odd order $v_n$ were observed later.
The large anisotropic flow suggests a substantial role of final state interaction after the initial production.
In the hydrodynamic picture, non-central collisions results in an initial almond shape of the deposited energy density and the hydrodynamic pressure builds up and drives the transverse expansion of the fireball.
Because the pressure gradient is larger in the short axis-direction and the long-axis direction, the matter flows in an isotropic way, creating the observed momentum space anisotropy.
Higher order of flow harmonics is a result of the initial state fluctuation of the nuclear wave function.
In short, the hydrodynamics transfer initial geometry eccentricities $\epsilon_n$ into final state momentum anisotropy $v_n$.

A relativistic ideal hydrodynamic model (no viscous effect) assumes an infinitely strong interaction that the medium always stages in local thermal equilibrium.
A more realistic system with finite interact strength will go off-equilibrium under the driven force, and is treated in relativistic viscous hydrodynamics with shear viscosity $\eta$ and bulk viscosity $\zeta$ as inputs.
Because the viscous effects damp the spatial anisotopic of the flow, the efficiency of the transition from $\epsilon_n$ to $v_n$ is very sensitive to the values of shear and bulk viscosity and can be used for precision extraction of the these number from experimental measurements.

The QGP shear viscosity and bulk viscosity are of fundamental importance. 
The shear viscosity to entropy ratio $\eta/s$ is an indicator of the stong / weak coupling nature of the QGP. 
And the bulk viscosity to entropy ratio $\zeta/s$ is directly related to  the scale-invariance breaking of QCD.
Dynamical quantities such as viscosity are extremely hard to be computed from first principle, so currently, pinning down these numbers and their temperature / chemical potential dependences from experiments is essential in learning the dynamical properties of QGP.
Global comparisons of the state-of-the-art medium modeling to a collection of soft observables have corroborate the need of a small $\eta/s$ that are likely to be slowly increasing with temperature and a non-vanishing $\zeta/s$.

\subsection{Constituent quark scaling and the degrees freedom}
The hydrodynamics is successful on describing the collectivity, but does not assume what is the effective degrees of freedom that is flowing.

\section{Hard probes}
Very occasionally, particles with large transverse momentum ($p_T\gtrsim 10$ GeV) are produced, and there are referred to as the ``hard" particles.
By uncertainty principal, these hard particles can only be produced in the initial collisions on a time scale $\delta t \sim 1/p_T$.
Therefore, a hard process can only be produced at the very beginning of the nuclear collision and interacts with the nuclear medium that surrounds it.
Due to the large $p_T$ and relatively small coupling constant, the production of these hard particles can be studied in a perturbative framework.
The interaction with the medium modifies the initial production and leaves finger prints of the nuclear matter properties in the hard observables. 
So, these hard particles can be used as self-generated probes of the system.
\subsection{Jet quenching}
Jet quenching refers to the strong suppression of the yield of high transverse momentum hadrons in nuclear collisions, compared to the scaled yield in proton-proton collisions where medium effects are assumed to be small.
Calculations have shown that this suppression is a consequence of jets losing energy to the hot, dense and color-deconfined medium. 

\subsection{Heavy-flavors}
Heavy quarks (charm and bottom) are often seen as complementary probes of the QGP, but partly also belong to the category of jet observables, depending on their transverse momenta. Their large masses (compared to the prevailing temperatures generated in collisions at current heavy-ion colliders) constrain their production to early reaction times via hard perturbative Quantum-Chromodynamics (pQCD) processes. Flavor conservation ensures that the overwhelming majority of heavy quarks survive the entire reaction, allowing them to probe the full space-time evolution of the reaction.
These two features are particular attractive to theorists as these flavor-tagged particles are much easier to track in the calculations than the evolution of a full jet.
The mass also sets an additional energy scale to the problem and brings rich physics to the heavy-flavor sector.
In the high transverse momentum region, heavy quarks lose energy mainly through radiative processes connecting them to jet energy loss studies {Wicks:2007am, Djordjevic:2004nq, Xu:2014tda, Kang:2016ofv}, whereas
in the low transverse momentum region their large mass delays their thermalization, providing a window to study the equilibration process {Moore:2004tg,Riek:2010fk,Cao:2013ita}.
Heavy flavors are therefore ideal and unique probes to determine QGP properties.

The in-medium propagation of heavy quarks is often studied in a kinetic approach that is linearized with respect to the heavy quark distribution function and the medium particle distribution function is assumed to be thermal, obtained from hydrodynamic models.
The linearization implies that any effects of the heavy quark interactions on the medium are neglected.

The linearized Boltzmann transport equation and the Langevin equation are both widely used linearized models but make different assumptions regarding the nature of the interaction and thus often focus on different regimes in the heavy quark phase space {Auvinen:2009qm,Cao:2016gvr, Cao:2017hhk, PhysRevD.37.2484, Moore:2004tg}.
The linearized Boltzmann transport equation is based on elementary scattering processes that can be directly calculated, e.g. via pQCD.
However, calculations in the presence of a medium are extremely complicated even at leading order {Arnold:2002zm}.
Also, the pQCD processes are often plagued by soft divergences that need to be regulated by a medium scale proportional to temperature. Moreover, at current collision energies the relevant temperature is not high enough which creates ambiguities for the pQCD calculation through the scale dependence of the strong coupling constant $\alpha_s$.

The Langevin equation takes a different approach: 
it assumes that heavy quark receives frequent but soft momentum kicks from the medium, making a statistical description of the interaction possible -- in terms of ``drag" and ``diffusion" coefficients.
These transport coefficients encode the first and second moments of the momentum-exchange rate but are agnostic to further details of the elementary processes and medium properties.
There are efforts to calculate these transport coefficients in various approaches including lattice QCD {Moore:2004tg,CaronHuot:2008uh, Gossiaux:2008jv,He:2012df,Riek:2010fk,vanHees:2007me,Scardina:2017ipo,Ding:2012sp,Banerjee:2011ra,Francis:2015daa}. Our group has taken a complementary approach, using experiment data to calibrate our Langevin based transport model to measured observables and thus extract the transport coefficients directly from data via a Bayesian analysis {Xu:2017obm}. The drawback of this approach is that it does not in itself provide a fundamental understanding of the interaction mechanism but can only provide guidance to direct calculations of the transport coefficients in terms of compatibility to experimental observation.

\section{Understanding QGP through model-to-data comparison}
Immediately after the nuclei penetration through each other $t\sim 2R/\gamma$, huge amount of energy is deposited into the overlapped area and entropy is produced, creating a fireball in the middle while the nuclear remnants recede.
This highly excited fireball of fields undergoes complex dynamics and cools down rapidly because of the longitudinal and transverse expansion.
Eventually, it reaches to a point where the temperature is low enought that the quarks and gluons are again confined into hadrons. 
The hadrons can have further interactions and may decay into other hadrons, photons and lepton.
The dynamical of QGP we are interested in only lasts for $O(10) fm/$c, while we can only observe the remnants at the end of the collisions.
Therefore, the understanding of the QGP requires both an accurate dynamical modeling of the entire process and the design of sensitive observables.


\paragraph{Complex dynamical model with high-dimensional inputs}
Taking the bulk model as an example, it includes an initial condition, a dynamical model for pre-equilibrium stage, the viscous hydrodynamics, particlization of the hydrodynamic energy momentum tensor and the hadronic decays and rescatterings.
Each of the module takes both physical parameters and modeling parameters.
Eventually, one can have more than 10 parameters.
To generate one event with a model itself is already computationally intensive, while to be able to compare to experimental measurements that average over centrality with a good control of statistical uncertainty, $O(10^4)$ minimum-biased events needed to be generated.
As a result, a na\"ive linear grid sweep over the 10-dimensional parameter space is certainty not feasible.
To solve this problem of high-dimensional input complex models requires advanced parameter set design method and reliable interpolation schemes.


\paragraph{Model uncertainty and correlation among parameters}
We are mostly interested in those ``physical parameters / quantities'', for example, the transport coefficients $\eta/s(T)$ and $\zeta/s(T)$ of the medium.
Other parameters, such the switching time between the pre-equilibrium free-streaming dynamics and the hydrodynamics, and the temperature at which one particlize a hydrodynamic field description into a hadronic ensemble are less interested in the physical sense.
The appearance of these matching parameters are simply due to we do not have a single ``global model'' that works reasonably well under all circumstances.
However, one should not think less of these model parameters, because the quantify a significant fraction of the modeling uncertainty.
The model uncertainty can affect the interpretation of the experimental data, and any quantitative statement we draw about those physical quantities from a model-to-data comparison.
For example, the  bulk particles anisotropic flows, though proved to be very sensitive to the shear viscosity, are also sensitive to the initial eccentricity of the initial conditions.
Therefore, the exacted $\eta/s$ becomes correlated with the choice of initial condition models and its parameters.
The model uncertainty is hard problem, and one can either try to reduce it by improving the model or quantify it when compared to data to prevent an over-interpretation.

\paragraph{A high-dimensional model output / observables}
The inclusion of more observables also helps to break the model degeneracy and parameter correlation.
But it can be tricky to quantify the quality of agreement between model and data with a collection of observables (a high dimensional output).


The Bayesian analysis framework is introduced to the field by the earily works of and the PhD thesis of Jonah Bernhard [] who has successfully applied the tool to the extraction of the initial condition topology and temperature dependent QGP transport coefficient.
This chapter provides a concise description of the Bayesian analysis and for full details please refer to the thesis of Jonah Bernhard [].